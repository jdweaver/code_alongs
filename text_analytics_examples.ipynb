{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "*Inspired by and adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Natural Language Processing, a programmatic process using computers or the cloud to prepare, process, analyze, understand, and generate human language that occurs \"in the wild\" e.g. tweets, blog posts, emails etc. \n",
    "- Text data is UNstructured, it does not occur as tidy columns and rows\n",
    "- Because speech is unstructured text. We need a way to make sense of it. \n",
    "- We can probabilistically model language with machines by converting language into using machine understable data\n",
    "\n",
    "### How do we get computers to analyze spoken or written human language? \n",
    "- we represent the language as numerical features that we can then analyze. We do this because most algorithms expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "- from the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction): \n",
    "    - **tokenizing:** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "    - **counting:** the occurrences of tokens in each document.\n",
    "    - **normalizing and weighting:** with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)\n",
    "- **Analyzing job descriptions to reach diverse candidates:** \n",
    "    - [Textio](https://textio.com/products/) \n",
    "- **Determining personality using text analytics:**\n",
    "    - [via Facebook (no not cambridge analytica)](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0073791)\n",
    "- **Diagnosing mental health using text analytics:**\n",
    "    - [A relatively early application before the cloud. But a great example of using text analysis to agument human judgement and in a relatively \"squishy\" area--psychology](https://search.proquest.com/openview/0eb8bfa741f62292f1900dfecd011ed8/1?pq-origsite=gscholar&cbl=40661) \n",
    "- **Natural Languge Understanding:** training a machine to process (understand) what humans say. Realtime or otherwise. A good example of NLU is the [Google Assistant demo](https://www.theverge.com/2018/5/8/17332070/google-assistant-makes-phone-call-demo-duplex-io-2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the. These are words with high frequency that introduce noise into the analysis. \n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "In short, humans are good at handling mispronunciations, \"improper\" grammar, swapped words, contractions, colloquialisms, puns, and other variations etc. \n",
    "\n",
    "Machines are not. They are still toasters (drew carey points if you get that reference!)...much less capable of working with unpredictable inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we need to install some packages\n",
    "conda install -c https://conda.anaconda.org/sloria textblob\n",
    "\n",
    "cona install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to install some existing text corpora from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "'''CountVectorizer, and TfidfVectorizer are what we will use \n",
    "to \"convert text into a matrix of token counts\"\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "path = '/Users/Joshua/Downloads/'\n",
    "data = 'yelp.csv'\n",
    "yelp = pd.read_csv(path+data)\n",
    "\n",
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "6  zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "6  Drop what you're doing and drive here. After I...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
       "6  wFweIWhv2fREZV_dYkz_1g     7       7      4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_best_worst.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4086,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    My wife took me here on my birthday for breakf...\n",
       "1    I have no idea why some people give bad review...\n",
       "3    Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
       "4    General Manager Scott Petello is a good egg!!!...\n",
       "6    Drop what you're doing and drive here. After I...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4086,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    5\n",
       "3    5\n",
       "4    5\n",
       "6    5\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** breakdown text into component parts\n",
    "- **Why:** allows you to make unstructured data, structured and text data must be structured for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's look at the vector options we can set\n",
    "vect = CountVectorizer() #instantiate CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the [CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform)**: Learn the vocabulary dictionary and return term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Rows are documents (i.e. comments, tweets, blogs etc.), columns are terms (aka \"tokens\" or \"features\"). This creates a\n",
    "wide and extremely sparse dataframe or document term matrix. \n",
    "''' \n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00', u'000', u'00a', u'00am', u'00pm', u'01', u'02', u'03', u'03342', u'04', u'05', u'06', u'07', u'09', u'0buxoc0crqjpvkezo3bqog', u'0l', u'10', u'100', u'1000', u'1000x', u'1001', u'100th', u'101', u'102', u'105', u'1070', u'108', u'10am', u'10ish', u'10min', u'10mins', u'10minutes', u'10pm', u'10th', u'10x', u'11', u'110', u'1100', u'111', u'111th', u'112', u'115th', u'118', u'11a', u'11am', u'11p', u'11pm', u'12', u'120', u'128i']\n",
      "[u'yyyyy', u'z11', u'za', u'zabba', u'zach', u'zam', u'zanella', u'zankou', u'zappos', u'zatsiki', u'zen', u'zero', u'zest', u'zexperience', u'zha', u'zhou', u'zia', u'zihuatenejo', u'zilch', u'zin', u'zinburger', u'zinburgergeist', u'zinc', u'zinfandel', u'zing', u'zip', u'zipcar', u'zipper', u'zippers', u'zipps', u'ziti', u'zoe', u'zombi', u'zombies', u'zone', u'zones', u'zoning', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# first 50 features\n",
    "print vect.get_feature_names()[:50]\n",
    "\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'youthful', u'youtube', u'yow', u'yr', u'yrs', u'yu', u'yuck', u'yucky', u'yuk', u'yukon', u'yum', u'yumm', u'yummie', u'yummier', u'yumminess', u'yummm', u'yummmm', u'yummmmmmers', u'yummmmy', u'yummy', u'yumness', u'yup', u'yuppies', u'yuuuuummmmmyyy', u'yuyuyummy', u'yuzu', u'yyyyy', u'za', u'zen', u'zero', u'zest', u'zhou', u'zilch', u'zinc', u'zing', u'zip', u'zipper', u'ziti', u'zone', u'zones', u'zoning', u'zoo', u'zucchini', u'zuchinni', u'zupa', u'zwiebel', u'zzed', u'\\xc9cole', u'\\xe9clairs', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameter **ngram_range:** tuple (min_n, max_n)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a set of unigrams, bigrams, and trigrams\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "vect2 = CountVectorizer(ngram_range=(1,2))\n",
    "vect3 = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_train_dtm1 = vect1.fit_transform(X_train)\n",
    "X_train_dtm2 = vect2.fit_transform(X_train)\n",
    "X_train_dtm3 = vect3.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 456398)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 10 features of unigrams\n",
    "print vect1.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "# last 10 features of bigrams \n",
    "print vect2.get_feature_names()[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zzed in my', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9clairs napoleons and', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9cole len\\xf4tre trained', u'\\xe9m', u'\\xe9m all', u'\\xe9m all they']\n"
     ]
    }
   ],
   "source": [
    "# last 10 features of trigrams \n",
    "print vect3.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting the star rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of a SVM with gamma of 0.001 and C of 1: 0.873776908023\n",
      "F1 score of a SVM with gamma of 0.001 and C of 1: 0.494117647059\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use a Support Vector Machine to predict the star rating\n",
    "from sklearn import svm\n",
    "svm = svm.SVC(gamma=0.001, C=1)\n",
    "svm.fit(X_train_dtm, y_train)\n",
    "y_pred_class = svm.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print 'Accuracy of a SVM with gamma of 0.001 and C of 1:', metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "print 'F1 score of a SVM with gamma of 0.001 and C of 1:',metrics.f1_score(y_test, y_pred_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "best     0.819961\n",
       "worst    0.180039\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test_category = pd.DataFrame(np.where(y_test==5, 'best', 'worst'), columns = ['rating'])\n",
    "y_test_category.rating.value_counts() / len(y_test_category.rating) \n",
    "#calculate the average of each category, all thing absent the mean is the most basic way to model a set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates precision \n",
    "'''Recall that precision answers the question: What proportion of positive identifications was actually correct?\n",
    "Represented as: True Positives/(True Positives + False Positives)\n",
    "'''\n",
    "from sklearn import svm\n",
    "\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print 'Features: ', X_train_dtm.shape[1]\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    clf = svm.SVC(gamma=0.001, C=1)\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = clf.predict(X_test_dtm)\n",
    "    print 'Precision: ', metrics.precision_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16825\n",
      "Precision:  0.887323943662\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16825\n",
      "Precision:  0.887323943662\n"
     ]
    }
   ],
   "source": [
    "# include unigrams, default for CountVectorizer()\n",
    "vect1 = CountVectorizer(ngram_range=(0, 1))\n",
    "tokenize_test(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  169847\n",
      "Precision:  0.898550724638\n"
     ]
    }
   ],
   "source": [
    "# include bigrams\n",
    "vect2 = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "print(vect2.get_feature_names()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16825\n",
      "Precision:  0.887323943662\n"
     ]
    }
   ],
   "source": [
    "# include trigrams\n",
    "vect5 = CountVectorizer(ngram_range=(1, 10))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zzed in my', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9clairs napoleons and', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9cole len\\xf4tre trained', u'\\xe9m', u'\\xe9m all', u'\\xe9m all they']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\\nhttps://github.com/jupyter/notebook/issues/2287\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect3 = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_train_dtm3 = vect3.fit_transform(X_train)\n",
    "\n",
    "print(vect3.get_feature_names()[-10:])\n",
    "\n",
    "'''jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "https://github.com/jupyter/notebook/issues/2287\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok stop, \n",
    "Why is our precision so similar across our ngram types? \n",
    "Let's take a look at our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\\n\\nAnyway, I can't wait to go back!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1) #this lets us print out the contents of an entire cell\n",
    "print(yelp_best_worst.text[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular review is about 900 characters with spaces, about 3x a tweet but not a 500 word blog post either. The point is how sparse the data is, once created as a document term matrix it's pretty sparse, and in these examples we are letting scikit learn set the number of features, which is about 16825.\n",
    "\n",
    "This would be different if we processed the text differently, forced a particular set of features..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16528\n",
      "Precision:  0.866666666667\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'your', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100\n",
      "Precision:  0.888888888889\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'area', u'atmosphere', u'awesome', u'bad', u'bar', u'best', u'better', u'big', u'came', u'cheese', u'chicken', u'clean', u'coffee', u'come', u'day', u'definitely', u'delicious', u'did', u'didn', u'dinner', u'don', u'eat', u'excellent', u'experience', u'favorite', u'feel', u'food', u'free', u'fresh', u'friendly', u'friends', u'going', u'good', u'got', u'great', u'happy', u'home', u'hot', u'hour', u'just', u'know', u'like', u'little', u'll', u'location', u'long', u'looking', u'lot', u'love', u'lunch', u'make', u'meal', u'menu', u'minutes', u'need', u'new', u'nice', u'night', u'order', u'ordered', u'people', u'perfect', u'phoenix', u'pizza', u'place', u'pretty', u'prices', u'really', u'recommend', u'restaurant', u'right', u'said', u'salad', u'sandwich', u'sauce', u'say', u'service', u'staff', u'store', u'sure', u'table', u'thing', u'things', u'think', u'time', u'times', u'took', u'town', u'tried', u'try', u've', u'wait', u'want', u'way', u'went', u'wine', u'work', u'worth', u'years']\n"
     ]
    }
   ],
   "source": [
    "# all 100 features\n",
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100000\n",
      "Precision:  0.901408450704\n"
     ]
    }
   ],
   "source": [
    "# repeat with bigrams, and limit the number of features to 100,000\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  30000\n",
      "Precision:  0.905405405405\n"
     ]
    }
   ],
   "source": [
    "# include bigrams, and limit the number of features 30,000\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=30000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer parameter: \n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary ignore terms that have a document frequency lower than a given threshold. This value. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  30000\n",
      "Precision:  0.905405405405\n"
     ]
    }
   ],
   "source": [
    "# include bigrams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2),  max_features=30000, min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  14113\n",
      "Precision:  0.906666666667\n"
     ]
    }
   ],
   "source": [
    "# include bigrams, and only include terms that appear at least 5 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2),  max_features=30000, min_df=5)\n",
    "tokenize_test(vect)\n",
    "#note that there are not 30K bigrams that match the min_df constraint\n",
    "#also note that the precision increased to .906"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\r\n",
      "\r\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\r\n",
      "\r\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\r\n",
      "\r\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print yelp_best_worst.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'wife', u'took', u'me', u'here', u'on', u'my', u'birthday', u'for', u'breakfast', u'and', u'it', u'was', u'excel', u'the', u'weather', u'was', u'perfect', u'which', u'made', u'sit', u'outsid', u'overlook', u'their', u'ground', u'an', u'absolut', u'pleasur', u'our', u'waitress', u'was', u'excel', u'and', u'our', u'food', u'arriv', u'quick', u'on', u'the', u'semi-busi', u'saturday', u'morn', u'it', u'look', u'like', u'the', u'place', u'fill', u'up', u'pretti', u'quick', u'so', u'the', u'earlier', u'you', u'get', u'here', u'the', u'better', u'do', u'yourself', u'a', u'favor', u'and', u'get', u'their', u'bloodi', u'mari', u'it', u'was', u'phenomen', u'and', u'simpli', u'the', u'best', u'i', u've', u'ever', u'had', u'i', u\"'m\", u'pretti', u'sure', u'they', u'onli', u'use', u'ingredi', u'from', u'their', u'garden', u'and', u'blend', u'them', u'fresh', u'when', u'you', u'order', u'it', u'it', u'was', u'amaz', u'while', u'everyth', u'on', u'the', u'menu', u'look', u'excel', u'i', u'had', u'the', u'white', u'truffl', u'scrambl', u'egg', u'veget', u'skillet', u'and', u'it', u'was', u'tasti', u'and', u'delici', u'it', u'came', u'with', u'2', u'piec', u'of', u'their', u'griddl', u'bread', u'with', u'was', u'amaz', u'and', u'it', u'absolut', u'made', u'the', u'meal', u'complet', u'it', u'was', u'the', u'best', u'toast', u'i', u've', u'ever', u'had', u'anyway', u'i', u'ca', u\"n't\", u'wait', u'to', u'go', u'back']\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print [stemmer.stem(word) for word in review.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'wa', 'excellent', 'The', 'weather', u'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', u'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', u'egg', 'vegetable', 'skillet', 'and', 'it', u'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', u'piece', 'of', 'their', 'griddled', 'bread', 'with', u'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', u'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print [word.lemmatize() for word in review.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', u'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'be', 'excellent', 'The', 'weather', u'be', 'perfect', 'which', u'make', u'sit', 'outside', u'overlook', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'be', 'excellent', 'and', 'our', 'food', u'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', u'look', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', u'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'be', u'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', u'have', 'the', 'white', 'truffle', u'scramble', u'egg', 'vegetable', 'skillet', 'and', 'it', u'be', 'tasty', 'and', 'delicious', 'It', u'come', 'with', '2', u'piece', 'of', 'their', u'griddle', 'bread', 'with', u'be', u'amaze', 'and', 'it', 'absolutely', u'make', 'the', 'meal', 'complete', 'It', u'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', u'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print [word.lemmatize(pos='v') for word in review.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16452\n",
      "Accuracy:  0.875733855186\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zipps', u'ziti', u'zoe', u'zombi', u'zombie', u'zone', u'zoning', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel-kr\\xe4uter', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 20 features\n",
    "print vect.get_feature_names()[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF, what is it and why it matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Using TF-IDF to Summarize a Yelp Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP SCORING WORDS:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print word\n",
    "    \n",
    "    # print 5 random words\n",
    "    print '\\n' + 'RANDOM WORDS:'\n",
    "    random_words = np.random.choice(word_scores.keys(), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print word\n",
    "    \n",
    "    # print the review\n",
    "    print '\\n' + review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "game\n",
      "watch\n",
      "wooden\n",
      "flatbread\n",
      "fish\n",
      "\n",
      "RANDOM WORDS:\n",
      "sharp\n",
      "sign\n",
      "drinks\n",
      "bartender\n",
      "know\n",
      "\n",
      "This is an excellent place to watch a game. If I had only been there to watch a game I might give this place about 4 stars, but I'd had a long day, wanted to watch a game and unwind with a couple glasses of wine and some dinner.\r\n",
      "\r\n",
      "I stopped in recently with a friend when  in the area and Roka seemed less than lively. Our guess that this place might be more lively was perhaps correct, in that it had 6 people at the bar rather than 4 and had about 30 flat screen TV's running baseball games and various other sporting events. We chose to sit at the bar, because I always think you get better service there and the restaurant itself was pretty slow at 730 on a weeknight. That thing about better service didn't really hold up here as it seemed out bartender was also handling issues around the restaurant and after I finished my first glass of wine the glass sat empty for quite some time before someone came by to see about a refill. I will tolerate just about anything as long as I have a cocktail in front of me, and as pouring drinks is the primary function of a bartender this was more than slightly annoying. \r\n",
      "\r\n",
      "We ordered the ahi appetizer for a starter (I know I should probably be slapped for even ordering something like this in a sports bar). I had a couple bites and they were okay, but parts of the fish were actually starting to grey, which can't be a good sign. For dinner we had the salmon and a flatbread. \r\n",
      "\r\n",
      "Both the bartenders on duty raved about the flatbreads. I believe they have never eaten a flatbread anywhere else or they probably would stop raving about these. It was a relatively flavorless bread, which was not at all crisp on the bottom or the edges, lightly topped with cheese, sauce and some bits of chicken. It was edible as a food dish, but the part I found off putting was that the bread seemed to be taking on the flavor of the wooden paddle it was served upon. Yes, I know you can cook bread over wood and it will take on the wood flavor, but I am 99% sure this was coming off the wooden serving board. I actually took most of it home and the taste went away by the next day and the whole thing was well served by about 5 minutes in the toaster oven.  \r\n",
      "\r\n",
      "The salmon was also very very okay. The best thing I  think we ate was the broccoli side that came out with the fish. They were sharp green, flavorful and cooked perfectly. The fish was kind of overdone and uninteresting. I feel like maybe this place should go to a smaller menu to be able to have more consistency with preparation.\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  16528\n",
      "Accuracy:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  10000\n",
      "Accuracy:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english',max_features=10000)\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  sentiment  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0   0.402469  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0   0.229773  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0   0.566667  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0   0.608646  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0   0.468125  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1112e79d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEcCAYAAAAGD4lRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEFKGQsdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1257ba0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>106JT5p8e8Chtd0CZpcARw</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>KowGVoP_gygzdSu6Mt3zKQ</td>\n",
       "      <td>5</td>\n",
       "      <td>RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!</td>\n",
       "      <td>review</td>\n",
       "      <td>jKeaOrPyJ-dI9SNeVqrbww</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>-0.302083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id        date               review_id  stars  \\\n",
       "390  106JT5p8e8Chtd0CZpcARw  2009-08-06  KowGVoP_gygzdSu6Mt3zKQ      5   \n",
       "\n",
       "                                                                                                                                                                                text  \\\n",
       "390  RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!   \n",
       "\n",
       "       type                 user_id  cool  useful  funny  length  sentiment  \n",
       "390  review  jKeaOrPyJ-dI9SNeVqrbww     1       0      0     175  -0.302083  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>Gi-4O3EhE175vujbFGDIew</td>\n",
       "      <td>1</td>\n",
       "      <td>If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.</td>\n",
       "      <td>review</td>\n",
       "      <td>Hqgx3IdJAAaoQjvrUnbNvw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "1781  53YGfwmbW73JhFiemNeyzQ  2012-06-22  Gi-4O3EhE175vujbFGDIew      1   \n",
       "\n",
       "                                                                                                                         text  \\\n",
       "1781  If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.   \n",
       "\n",
       "        type                 user_id  cool  useful  funny  length  sentiment  \n",
       "1781  review  Hqgx3IdJAAaoQjvrUnbNvw     0       1      2     119   0.766667  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print X_train_dtm.shape\n",
    "print X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16829)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16829)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917808219178\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922700587084\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field\n",
    "- Understanding the basics broadens the types of data you can work with\n",
    "- Simple techniques go a long way\n",
    "- Use scikit-learn for NLP whenever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
